{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(preds, actuals):\n",
    "    preds = preds.reshape(-1)\n",
    "    actuals = actuals.reshape(-1)\n",
    "    assert preds.shape == actuals.shape\n",
    "    return 100 * np.linalg.norm((actuals - preds) / actuals) / np.sqrt(preds.shape[0])\n",
    "\n",
    "def test_da_shit(actuals, preds):\n",
    "    new_test= pd.DataFrame({'Actuals': actuals,'Preds': preds})\n",
    "    new_test = new_test.loc[new_test['Actuals'] != 0,:]\n",
    "    return metric(np.array(new_test['Actuals']), np.array(new_test['Preds']))\n",
    "\n",
    "def mean_encoder(df, col, target = 'Sales'):\n",
    "    Mean_encoded_subject = df.groupby([col])[target].mean().to_dict() \n",
    "    return df[col].map(Mean_encoded_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', parse_dates = True)\n",
    "stores = pd.read_csv('data/store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_raw = pd.merge(train, stores, how='outer', on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "raw = full_raw.Open.loc[(full_raw.Sales == 0)] = 0\n",
    "raw = full_raw.Open.loc[(full_raw.Sales > 0)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = full_raw.loc[~((full_raw.Sales.isnull()) | (full_raw.Store.isnull()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "raw.Date = pd.to_datetime(raw.Date, format='%Y-%m-%d')\n",
    "raw.DayOfWeek = raw.Date.dt.weekday + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_open = raw[~(raw.Open==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_means = raw_open.groupby(['Store', 'DayOfWeek']).mean()['Customers']\n",
    "customer_nulls = raw_open.loc[raw_open['Customers'].isnull()]\n",
    "for i, row in customer_nulls.iterrows():\n",
    "    store = row['Store']\n",
    "    DayOfWeek = row['DayOfWeek']\n",
    "    mean = customer_means.loc[store, DayOfWeek]\n",
    "    raw_open.loc[i, 'Customers'] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_open.reset_index(inplace = True, drop = True)\n",
    "null_promo = raw_open.loc[raw_open['Promo'].isnull()]\n",
    "\n",
    "for i, row in null_promo.iterrows():\n",
    "    above_and_below = raw_open.loc[i-1, 'Promo'] + raw_open.loc[i+1, 'Promo']\n",
    "    if above_and_below == 2:\n",
    "        raw_open.loc[i, 'Promo'] = 1\n",
    "    elif above_and_below == 0:\n",
    "        raw_open.loc[i, 'Promo'] = 0\n",
    "        \n",
    "null_promo = raw_open.loc[raw_open['Promo'].isnull()]\n",
    "\n",
    "for i, row in null_promo.iterrows():\n",
    "    date = str(row['Date'])\n",
    "    mean = raw_open.groupby('Date').mean().loc[date, 'Promo']\n",
    "    if mean > 0.9:\n",
    "        raw_open.loc[i, 'Promo'] = 1\n",
    "    else:\n",
    "        raw_open.loc[i, 'Promo'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_nulls = raw_open.loc[raw_open['StateHoliday'].isnull()]\n",
    "for i, row in holiday_nulls.iterrows():\n",
    "    date = str(row['Date'])\n",
    "    mode = raw_open.loc[raw_open['Date'] == date, 'StateHoliday'].mode()[0]\n",
    "    raw_open.loc[i, 'StateHoliday'] = mode\n",
    "    \n",
    "holiday_nulls = raw_open.loc[raw_open['SchoolHoliday'].isnull()]\n",
    "for i, row in holiday_nulls.iterrows():\n",
    "    date = str(row['Date'])\n",
    "    mode = raw_open.loc[raw_open['Date'] == date, 'SchoolHoliday'].mode()[0]\n",
    "    raw_open.loc[i, 'SchoolHoliday'] = mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_open_stores = raw_open.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.534890461313807"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "X = clean_open_stores[['Store', 'DayOfWeek']]\n",
    "y = clean_open_stores['Sales']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "lazy_est = pd.concat([X_train, y_train], axis=1)\n",
    "lazy_est = lazy_est.groupby(['Store', 'DayOfWeek']).mean()\n",
    "preds = X_test.merge(lazy_est, how = 'left', on = ['Store', 'DayOfWeek']).loc[:, 'Sales']\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Fixing the outlier (Store #817)\n",
    "clean_open_stores.Customers.loc[((clean_open_stores.Sales >= 20000) & (clean_open_stores.Customers >= 7000))] = clean_open_stores.loc[(clean_open_stores.Store == 817)].mean()['Customers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Store Type 'b' vs not 'b'\n",
    "clean_open_stores.loc[(clean_open_stores['StoreType'] == 'b'),'StoreType'] = 1\n",
    "clean_open_stores.loc[((clean_open_stores['StoreType'] == 'a')\n",
    "                      |(clean_open_stores['StoreType'] == 'c')\n",
    "                      |(clean_open_stores['StoreType'] == 'd')),'StoreType'] = 0\n",
    "clean_open_stores.StoreType = clean_open_stores.StoreType.astype('int64', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding State Holidays\n",
    "clean_open_stores.loc[((clean_open_stores['StateHoliday'] == 0) | (clean_open_stores['StateHoliday'] == 0.0)),'StateHoliday'] = 0\n",
    "clean_open_stores.loc[((clean_open_stores['StateHoliday'] == 'a') | (clean_open_stores['StateHoliday'] == 'b') | (clean_open_stores['StateHoliday'] == 'c')),'StateHoliday'] = 1\n",
    "clean_open_stores.StateHoliday = clean_open_stores.StateHoliday.astype('int64', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promo2 Length Engineering\n",
    "clean_open_stores['Promo2Time'] = 2015 - clean_open_stores['Promo2SinceYear']\n",
    "clean_open_stores.Promo2Time.loc[(clean_open_stores.Promo2Time == 2015)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Time Engineering\n",
    "clean_open_stores['CompetitionTime'] = 2015 - clean_open_stores['CompetitionOpenSinceYear']\n",
    "clean_open_stores.CompetitionTime.loc[(clean_open_stores.CompetitionTime == 2015)] = 0\n",
    "engineered_years = clean_open_stores.drop(columns = ['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear','Promo2SinceWeek', 'Promo2SinceYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortment Encoding\n",
    "engineered_years.Assortment.value_counts()\n",
    "engineered_years.loc[(engineered_years['Assortment'] == 'a'),'Assortment'] = 0\n",
    "engineered_years.loc[(engineered_years['Assortment'] == 'b'),'Assortment'] = 1\n",
    "engineered_years.loc[(engineered_years['Assortment'] == 'c'),'Assortment'] = 2\n",
    "engineered_years.Assortment = engineered_years.Assortment.astype('int64', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Distance Engineering Based on Quartiles\n",
    "engineered_years.loc[((engineered_years['CompetitionDistance'] > 0) & (engineered_years['CompetitionDistance'] <= 710)),'CompetitionDistance'] = 3\n",
    "engineered_years.loc[((engineered_years['CompetitionDistance'] > 710) & (engineered_years['CompetitionDistance'] <= 2320)),'CompetitionDistance'] = 2\n",
    "engineered_years.loc[((engineered_years['CompetitionDistance'] > 2320) & (engineered_years['CompetitionDistance'] <= 6880)),'CompetitionDistance'] = 1\n",
    "engineered_years.loc[((engineered_years['CompetitionDistance'] > 6880) & (engineered_years['CompetitionDistance'] <= 75860)),'CompetitionDistance'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = engineered_years.groupby('Store').sum()\n",
    "avg_purchase_per_store = gg['Sales'] / gg['Customers']\n",
    "avg_purchase_per_store = pd.DataFrame(avg_purchase_per_store)\n",
    "engineered_years = engineered_years.merge(avg_purchase_per_store, how = 'left', on = 'Store')\n",
    "engineered_years.rename({0: 'AvgPurchase'},axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set = engineered_years.drop(columns=['PromoInterval', 'Customers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = final_set.sample(frac = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.578430990441657"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest\n",
    "rf_set = sample.drop(['Date'], axis = 1)\n",
    "X = rf_set.drop(columns = 'Sales')\n",
    "y = rf_set['Sales']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.64440832420453"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest 2\n",
    "rf_set = sample.drop(['Date'], axis = 1)\n",
    "X = rf_set.drop(columns = 'Sales')\n",
    "y = rf_set['Sales']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "mask = y_train > 1200\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: 18.635237268998228\n",
      "45: 18.630090702998594\n",
      "50: 18.6186550449375\n",
      "55: 18.619428484888076\n",
      "60: 18.608626855268138\n",
      "65: 18.610431321468806\n",
      "70: 18.61328824369845\n",
      "75: 18.61259461745202\n",
      "80: 18.611675889567824\n",
      "85: 18.599917242762874\n",
      "90: 18.596008181389955\n",
      "95: 18.590771181284506\n",
      "100: 18.59580533170328\n",
      "105: 18.59648949830963\n",
      "110: 18.60447153467229\n",
      "115: 18.607110677606894\n"
     ]
    }
   ],
   "source": [
    "# Random forest 3\n",
    "rf_set = sample.drop(['Date'], axis = 1)\n",
    "X = rf_set.drop(columns = 'Sales')\n",
    "y = rf_set['Sales']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "mask = y_train > 500\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "for i in range(40,120,5):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    rf = RandomForestRegressor(n_estimators=i,random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    actuals = np.array(y_test)\n",
    "    print(f'{i}: {test_da_shit(actuals, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfinal_set.loc[((final_set['MeanPurchase'] > 0) & (final_set['MeanPurchase'] <= 7.79)),'MeanPurchase'] = 0\\nfinal_set.loc[((final_set['MeanPurchase'] > 7.79) & (final_set['MeanPurchase'] <= 9.14)),'MeanPurchase'] = 1\\nfinal_set.loc[((final_set['MeanPurchase'] > 9.14) & (final_set['MeanPurchase'] <= 10.79)),'MeanPurchase'] = 2\\nfinal_set.loc[((final_set['MeanPurchase'] > 10.79) & (final_set['MeanPurchase'] <= 64.96)),'MeanPurchase'] = 3\\n\""
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Purchase Engineering Based on Quartiles - DOES NOT HELP\n",
    "\"\"\"\n",
    "final_set.loc[((final_set['MeanPurchase'] > 0) & (final_set['MeanPurchase'] <= 7.79)),'MeanPurchase'] = 0\n",
    "final_set.loc[((final_set['MeanPurchase'] > 7.79) & (final_set['MeanPurchase'] <= 9.14)),'MeanPurchase'] = 1\n",
    "final_set.loc[((final_set['MeanPurchase'] > 9.14) & (final_set['MeanPurchase'] <= 10.79)),'MeanPurchase'] = 2\n",
    "final_set.loc[((final_set['MeanPurchase'] > 10.79) & (final_set['MeanPurchase'] <= 64.96)),'MeanPurchase'] = 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.274149771262525"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost Starting Point\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "xgbr = xgb.XGBRegressor(max_depth=4,learning_rate=0.05,subsample=0.9,n_estimators=1000,n_jobs=1,random_state=42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "preds = xgbr.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.761977526710798"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost 2 = no subsample, increased learning rate\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "xgbr = xgb.XGBRegressor(max_depth=4,learning_rate=0.1,n_estimators=1000,n_jobs=1,random_state=42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "preds = xgbr.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.144127513370915"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost 3 = no subsample, increased max depth\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=0.1,n_estimators=1000,n_jobs=1,random_state=42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "preds = xgbr.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.538720238403954"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost 4 = no subsample, decreased max depth\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "xgbr = xgb.XGBRegressor(max_depth=3,learning_rate=0.1,n_estimators=1000,n_jobs=1,random_state=42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "preds = xgbr.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05: 19.88254155000993\n",
      "0.1: 18.144127513370915\n",
      "0.15: 17.667663169002832\n",
      "0.2: 17.436614375959362\n",
      "0.25: 17.393457027331674\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 4 = max_depth=5, test learning rates\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "for i in range(5,30,5):    \n",
    "    xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=(i/100),n_estimators=1000,n_jobs=1,random_state=42)\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    preds = xgbr.predict(X_test)\n",
    "    actuals = np.array(y_test)\n",
    "    test_da_shit(actuals, preds)\n",
    "    print(f'{i/100}: {test_da_shit(actuals, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15: 17.667663169002832\n",
      "0.2: 17.436614375959362\n",
      "0.25: 17.393457027331674\n",
      "0.3: 17.45956217116159\n",
      "0.35: 17.484324979648672\n",
      "0.4: 17.496922963113466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-8596765efcf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mxgbr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mxgbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[1;32m    540\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# XGBoost 5 = max_depth=5, test learning rates further\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "for i in range(15,50,5):    \n",
    "    xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=(i/100),n_estimators=1000,n_jobs=1,random_state=42)\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    preds = xgbr.predict(X_test)\n",
    "    actuals = np.array(y_test)\n",
    "    test_da_shit(actuals, preds)\n",
    "    print(f'{i/100}: {test_da_shit(actuals, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: 17.950787921112976\n",
      "1000: 17.393457027331674\n",
      "1500: 17.369687524884824\n",
      "2000: 17.446905026609933\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 6 = max_depth=5, learning_rate=0.25 , test n_estimators (1500 gives the best result), takes super long to run, do not repeat\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "for i in range(500,2500,500):    \n",
    "    xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=0.25,n_estimators=i,n_jobs=1,random_state=42)\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    preds = xgbr.predict(X_test)\n",
    "    actuals = np.array(y_test)\n",
    "    test_da_shit(actuals, preds)\n",
    "    print(f'{i}: {test_da_shit(actuals, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 7 = max_depth=5, learning_rate=0.3, n_estimators=1500, test subsampling\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "for i in range(8,10,1):    \n",
    "    xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=0.3,n_estimators=1500,n_jobs=1,random_state=42,subsample=(i/10))\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    preds = xgbr.predict(X_test)\n",
    "    actuals = np.array(y_test)\n",
    "    test_da_shit(actuals, preds)\n",
    "    print(f'{i/10}: {test_da_shit(actuals, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 8 = max_depth=5, learning_rate=0.3, n_estimators=1500, test colsample_bytree\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "for i in range(3,9,1):    \n",
    "    xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=0.3,n_estimators=1500,n_jobs=1,random_state=42,colsample_bytree=(i/10))\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    preds = xgbr.predict(X_test)\n",
    "    actuals = np.array(y_test)\n",
    "    test_da_shit(actuals, preds)\n",
    "    print(f'{i/10}: {test_da_shit(actuals, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.769798009180715"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost 9 = increased max depth\n",
    "boost_set = sample.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "xgbr = xgb.XGBRegressor(max_depth=6,learning_rate=0.25,n_estimators=1500,n_jobs=1,random_state=42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "preds = xgbr.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 10 = max_depth=5, learning_rate=0.25, n_estimators=1500 - use all of test data\n",
    "boost_set = final_set.drop(columns='Date')\n",
    "X = boost_set.drop(columns='Sales')\n",
    "y = boost_set.Sales\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "   \n",
    "xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=0.25,n_estimators=1500,n_jobs=1,random_state=42)\n",
    "xgbr.fit(X_train, y_train)\n",
    "preds = xgbr.predict(X_test)\n",
    "actuals = np.array(y_test)\n",
    "test_da_shit(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
